{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWD2QvC22_0W"
   },
   "source": [
    "# 0. Marking.\n",
    "\n",
    "***IMPORTANT***: Save a copy of this notebook into your Drive before you start.\n",
    "- Please attempt all the questions marked for your group (Part II ✅ | Part III/MPhil ✅).\n",
    "\n",
    "Please submit a zip file, containing both parts, consiting of of:\n",
    "1. A text file with a publicly visible link to your notebooks in Google Colab or GitHub.\n",
    "2. A downloaded copy (ipynb) of your notebooks or your zipped cloned GitHub repo. You may treat these as a report: we will not be re-executing the code you used to produce the answers unless required.\n",
    "\n",
    "If you find yourself enjoying the material, feel free to attempt more! Provide your answers in a new cell below the question cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RVKTUExooKZ"
   },
   "source": [
    "# 1. Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZX87_5hsydM"
   },
   "source": [
    "Welcome to the second lab session in our FL course.\n",
    "We have already explored how to “federate” a centralized ML model.\n",
    "You should now be able to understand who are the main actors in an FL orchestration.\n",
    "\n",
    "In this lab, we will go a step further using all the tools we learned previously.\n",
    "This lab will focus on the behaviour of an FL system under **heterogeneity**.\n",
    "You may know that heterogeneity represents both an intrinsic property and a challenge in a real FL setting.\n",
    "The literature agrees with splitting this property into two main categories.\n",
    "\n",
    "- We will refer to **system heterogeneity** when the clients, or more generally the participants, of the federation, have very different hardware from each other. This may condition clients' availability and participation in FL rounds. _NOTE: the availability of clients could also depend on network conditions that are independent of the device's hardware. This particular heterogeneity will fall in this category as well._ In this lab, we will mostly refer to **data heterogeneity**. This occurs in those settings in which clients' datasets are non-i.i.d. (non-independent and identically distributed), which is the most frequent case. Even if it is quite obvious, it is worth saying that real distributed collections of data are intrinsically non-i.i.d. Thus creating more than a few statistical concerns on the convergence of widely used algorithms which assume i.i.d. samples.\n",
    "\n",
    "_NOTE: for a more comprehensive discussion we suggest reading Sec.3.1 from [Advanced and Open Problems in Federated Learning](https://www.nowpublishers.com/article/Details/MAL-083)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b3v_pcscNZW"
   },
   "source": [
    "# 2. An overview of data heterogeneity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umXZe4Z2IWnh"
   },
   "source": [
    "Data heterogeneity has been extensively investigated and characterized in statistics and other fields. Here we want to start with a quick overview of the flavours that come to light in FL:\n",
    "\n",
    "- **Feature distribution skew**\n",
    "- **Label distribution skew**\n",
    "- **Quantity skew**\n",
    "- _Same label, different features_\n",
    "- _Same features, different label_\n",
    "- _Violation of Independence_\n",
    "- _Dataset Shift_\n",
    "\n",
    "During this lab we will get into more detail about the first three categories only---those bolded.\n",
    "\n",
    "It is crucial to say that a naturally partitioned dataset, also referred to as a \"real FL dataset”, usually presents a combination of more than one of these properties. This enforces developers to take into account **all of these** during the building of the FL pipeline because in real settings **we cannot see or touch data**. I like to exemplify such a situation using a metaphor: data in FL are like a Schroedinger's cat for which we cannot open the cage. Before looking into the details of these flavours, let's install and load some useful packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bbc-zZGqJDJB",
    "outputId": "32718d4f-1a1c-4f06-e1d3-79116d532f4c"
   },
   "outputs": [],
   "source": [
    "# The simulation component of flower uses RAY under the hood.\n",
    "# `pip` could produce some errors. Nothing to worry about.\n",
    "# The execution has been verified, it's working anyway.\n",
    "! pip install --quiet --upgrade \"pip\"\n",
    "! pip install --quiet git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching torch torchvision matplotlib gdown tqdm ray==\"2.6.3\" seaborn\n",
    "# The following is just needed to show the folder tree\n",
    "! apt-get install -qq tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99aC-H3gr5ey"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from collections.abc import Callable\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from logging import INFO\n",
    "from datetime import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "import flwr as fl\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "from flwr.common.parameter import ndarrays_to_parameters\n",
    "from flwr.common.typing import NDArrays, Parameters, Scalar\n",
    "from flwr.common.logger import log\n",
    "from flwr.server import ServerConfig, History\n",
    "from flwr.server.strategy import FedAvg, Strategy\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from enum import IntEnum\n",
    "from flwr.client import Client\n",
    "\n",
    "\n",
    "# Add new seeds here for easy autocomplete\n",
    "class Seeds(IntEnum):\n",
    "    DEFAULT = 1337\n",
    "\n",
    "\n",
    "np.random.seed(Seeds.DEFAULT)\n",
    "random.seed(Seeds.DEFAULT)\n",
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def fit_client_seeded(client, params, conf, seed=Seeds.DEFAULT):\n",
    "    \"\"\"Wrapper to always seed client training.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    return client.fit(params, conf)\n",
    "\n",
    "\n",
    "PathType = Path | str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smKbgZnrJGsX"
   },
   "source": [
    "Another important ingredient, necessary to analyse heterogeneity, is a labelled dataset. We will use FEMNIST since we are all familiar with it now. We will initially load the same version of the dataset used in the previous lab, then we will start creating partitions at will.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hQhnTM0JU9y"
   },
   "outputs": [],
   "source": [
    "home_dir = content if (content := Path(\"/content\")).exists() else Path.cwd()\n",
    "dataset_dir: Path = home_dir / \"femnist\"\n",
    "data_dir: Path = dataset_dir / \"data\"\n",
    "centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n",
    "centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
    "federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(o: Any) -> int | float:\n",
    "    \"\"\"Convert input object to Python numerical if numpy.\"\"\"\n",
    "    # type: ignore[reportGeneralTypeIssues]\n",
    "    if isinstance(o, np.int32 | np.int64):\n",
    "        return int(o)\n",
    "    # type: ignore[reportGeneralTypeIssues]\n",
    "    if isinstance(o, np.float32 | np.float64):\n",
    "        return float(o)\n",
    "    raise TypeError\n",
    "\n",
    "\n",
    "def save_history(hist: History, name: str) -> None:\n",
    "    \"\"\"Save history from simulation to file.\"\"\"\n",
    "    time = int(datetime.now(timezone.utc).timestamp())\n",
    "    path = home_dir / \"histories\"\n",
    "    path.mkdir(exist_ok=True)\n",
    "    path = path / f\"hist_{time}_{name}.json\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hist.__dict__, f, ensure_ascii=False, indent=4, default=convert)\n",
    "\n",
    "\n",
    "def start_seeded_simulation(\n",
    "    client_fn: Callable[[str], Client],\n",
    "    num_clients: int,\n",
    "    config: ServerConfig,\n",
    "    strategy: Strategy,\n",
    "    name: str,\n",
    "    seed: int = Seeds.DEFAULT,\n",
    "    iteration: int = 0,\n",
    ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
    "    \"\"\"Wrap simulation to always seed client selection.\"\"\"\n",
    "    np.random.seed(seed ^ iteration)\n",
    "    torch.manual_seed(seed ^ iteration)\n",
    "    random.seed(seed ^ iteration)\n",
    "    parameter_list, hist = fl.simulation.start_simulation_no_ray(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=num_clients,\n",
    "        client_resources={},\n",
    "        config=config,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    save_history(hist, name)\n",
    "    return parameter_list, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twoHcMsJ4n8T"
   },
   "outputs": [],
   "source": [
    "#  Download compressed dataset\n",
    "if not (home_dir / \"femnist.tar.gz\").exists():\n",
    "    file_id = \"1-CI6-QoEmGiInV23-n_l6Yd8QGWerw8-\"\n",
    "    gdown.download(\n",
    "        f\"https://drive.google.com/uc?export=download&confirm=pbef&id={file_id}\",\n",
    "        str(home_dir / \"femnist.tar.gz\"),\n",
    "    )\n",
    "\n",
    "# Decompress dataset\n",
    "if not dataset_dir.exists():\n",
    "    !tar -xf {str(home_dir)}/femnist.tar.gz -C {str(home_dir)} 2> /dev/null\n",
    "    log(INFO, f\"Dataset extracted in {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EVPpFjuwoBm"
   },
   "source": [
    "We also need the dataset object. We will use the same as in the previous lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (home_dir / \"common\").exists():\n",
    "    ! git clone \"https://github.com/camlsys/L361-Federated-Learning.git\" temp_repo\n",
    "\n",
    "    # Copy the folder to the current directory\n",
    "    ! cp -r \"temp_repo/labs/common\" {home_dir}\n",
    "\n",
    "    # Delete the cloned repository\n",
    "    ! rm -rf temp_repo\n",
    "\n",
    "    # Create the __init__.py file\n",
    "    (home_dir / \"__init__.py\").open(mode=\"a+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.femnist_dataset import FEMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLx7-YaNud_e"
   },
   "source": [
    "### 2.1 Feature distribution skew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P25sgH4ZH2Xg"
   },
   "source": [
    "Statisticians and data scientists refer to this property as \"covariate shift”. It happens when different clients present samples which describe ideally the same objects having slightly different features. That is the case for two different writers hand-writing the same word, since these may have different stroke widths, slants, etc.\n",
    "What happens to an FL setting in which this property is strong is not always predictable. Ideally, the global model, obtained using FedAvg, will try to learn a shared representation between clients. This could be problematic since clients have different features to represent their data. We could simply expect that the same seed model trained separately on local clients may perform better than a global model obtained in an FL training.\n",
    "\n",
    "To see this property in our chosen dataset we will show two images for the same number taken from different clients.\n",
    "_NOTE: the choice of clients and samples in the following is not random, but suitably made to show you the property._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "GQSu0BsBw_tO",
    "outputId": "ca9a8035-55f7-4112-af63-210f7c5bec35"
   },
   "outputs": [],
   "source": [
    "client_0_dataset: FEMNIST = FEMNIST(\n",
    "    mapping=federated_partition / \"0\", data_dir=data_dir, name=\"train\"\n",
    ")\n",
    "client_1000_dataset: FEMNIST = FEMNIST(\n",
    "    mapping=federated_partition / \"1000\", data_dir=data_dir, name=\"train\"\n",
    ")\n",
    "img_a, label_a = client_0_dataset[4]\n",
    "img_b, label_b = client_1000_dataset[0]\n",
    "\n",
    "log(INFO, f\"For client 0, sample 4 has label {label_a}\")\n",
    "log(INFO, f\"For client 1000, sample 0 has label {label_b}\")\n",
    "# display images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(img_a)\n",
    "ax[1].imshow(img_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8BULdP73HYV"
   },
   "source": [
    "We can also compare the average values of all the features for all the samples of these clients having the label chosen. You can try to execute the cell below choosing different labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "9xS7S-xk07dQ",
    "outputId": "f42a7c53-d12b-4c7c-8867-c7b99dceb976"
   },
   "outputs": [],
   "source": [
    "label_chosen = 4\n",
    "features_0 = []\n",
    "for img, lbl in client_0_dataset:\n",
    "    if lbl == label_chosen:\n",
    "        features_0.append(np.asarray(img).flatten())\n",
    "\n",
    "features_1000 = []\n",
    "for img, lbl in client_1000_dataset:\n",
    "    if lbl == label_chosen:\n",
    "        features_1000.append(np.asarray(img).flatten())\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow((np.sum(features_0, axis=0) / len(features_0)).reshape((28, 28)))\n",
    "ax[1].imshow((np.sum(features_1000, axis=0) / len(features_1000)).reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "KbPKX2SuWFcr",
    "outputId": "3edbabad-0e98-4f6e-9e6a-4e68536a5984"
   },
   "outputs": [],
   "source": [
    "label_chosen = 1\n",
    "features_0 = []\n",
    "for img, lbl in client_0_dataset:\n",
    "    if lbl == label_chosen:\n",
    "        features_0.append(np.asarray(img).flatten())\n",
    "\n",
    "features_1000 = []\n",
    "for img, lbl in client_1000_dataset:\n",
    "    if lbl == label_chosen:\n",
    "        features_1000.append(np.asarray(img).flatten())\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow((np.sum(features_0, axis=0) / len(features_0)).reshape((28, 28)))\n",
    "ax[1].imshow((np.sum(features_1000, axis=0) / len(features_1000)).reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MbYTin-77BR"
   },
   "source": [
    "It is worth having a glance on what are the consequences of this property on an FL experiment. To do that we will re-use the code for training an FL client from the previous lab.\n",
    "\n",
    "The following cell is thus meant to import objects and methods you have already used in the previous lab. There's nothing new in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb6M0AD4FdOP"
   },
   "outputs": [],
   "source": [
    "from common.client_utils import (\n",
    "    to_tensor_transform,\n",
    "    get_network_generator_mlp,\n",
    "    get_network_generator_cnn,\n",
    "    get_model_parameters,\n",
    "    get_federated_evaluation_function,\n",
    "    aggregate_weighted_average,\n",
    "    get_device,\n",
    ")\n",
    "from common.client import FlowerClient, get_flower_client_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_hZ8ZKqAwsY"
   },
   "source": [
    "In order to show on a small scale what can happen to FL training setups where clients have feature distribution skew, we will now build two toy settings. We choose the most populated client---the one with the most samples in the federation. We will then construct a second version of this same client by artificially changing its features by inverting the underlying images--- transforming each image into its negative. Furthermore, we need a specific `get_flower_client_generator` able to invert the images of a specific client.\n",
    "\n",
    "In the following, we provide the relevant methods to do that. These methods are simplified taking advantage of the federation being composed of just two clients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y420LYKH_8BL"
   },
   "outputs": [],
   "source": [
    "def load_FEMNIST_inverted_dataset(mapping: Path, name: str) -> Dataset:\n",
    "    \"\"\"Load the filterd FEMNIST dataset given the mapping .csv file.\n",
    "\n",
    "    The relevant transforms are automatically applied.\n",
    "    Note that the last transform will invert images, getting their negative\n",
    "    representation.\n",
    "\n",
    "    Args:\n",
    "        mapping (Path): path to the mapping .csv file chosen.\n",
    "        name (str): name of the dataset to load, train or test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Dataset: FEMNIST dataset object, ready to use.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.functional.invert,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FEMNIST(\n",
    "        mapping=mapping,\n",
    "        name=name,\n",
    "        data_dir=data_dir,\n",
    "        transform=transform,\n",
    "        target_transform=to_tensor_transform,\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: We need the self to use this for replacing an internal of a class\n",
    "\n",
    "\n",
    "def _load_inverted_dataset(self, name: str) -> Dataset:\n",
    "    full_file: Path = self.partition_dir / str(self.cid)\n",
    "    return load_FEMNIST_inverted_dataset(mapping=full_file, name=name)\n",
    "\n",
    "\n",
    "def get_mod_flower_client_generator(\n",
    "    model_generator: Callable[[], Module],\n",
    "    data_dir: Path,\n",
    "    partition_dir: Path,\n",
    "    mapping_fn: Callable[[int], int] | None = None,\n",
    ") -> Callable[[str], FlowerClient]:\n",
    "    \"\"\"Wrap the function for the client instance generator.\n",
    "\n",
    "    This provides the client generator with a model generator function.\n",
    "    Also, the partition directory must be passed.\n",
    "    A mapping function could be used for filtering/ordering clients.\n",
    "    Note that the \"even\" clients here will have a modified `_load_dataset` function.\n",
    "    The new `_load_dataset` has been chosen to be the one inverting the images.\n",
    "\n",
    "    Args:\n",
    "        data_dir (Path): path to the dataset folder.\n",
    "        model_generator (Callable[[], Module]): model generator function.\n",
    "        partition_dir (Path): directory containing the partition.\n",
    "        mapping_fn (Optional[Callable[[int], int]]): function mapping sorted/filtered\n",
    "            ids to real cid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Callable[[str], FlowerClient]: client instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> FlowerClient:\n",
    "        \"\"\"Create a single client instance given the client id `cid`.\n",
    "\n",
    "        Args:\n",
    "            cid (str): client id, Flower requires this to of type str.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            FlowerClient: client instance.\n",
    "        \"\"\"\n",
    "        log(INFO, f\"Getting client with id {cid}\")\n",
    "        actual_cid = mapping_fn(int(cid)) if mapping_fn is not None else int(cid)\n",
    "        client = FlowerClient(\n",
    "            cid=actual_cid,\n",
    "            data_dir=data_dir,\n",
    "            partition_dir=partition_dir,\n",
    "            model_generator=model_generator,\n",
    "        )\n",
    "        # Pay attention to the following two lines\n",
    "        if int(cid) % 2 == 0:\n",
    "            client._load_dataset = _load_inverted_dataset.__get__(client, FlowerClient)\n",
    "        return client\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6-ipEzaY8QY"
   },
   "source": [
    "We will now try to train separately the two clients generated this way. The mapping function will be provided to point both client IDs to the data of the most populated client.\n",
    "\n",
    "```python\n",
    "mapping_dict = {0: '178', 1: '178'}\n",
    "```\n",
    "\n",
    "_NOTE: you may want *to experiment \\_with \\_different\\_\\_ clients here*. After having completed the lab, feel free to try!_\n",
    "\n",
    "The following cell will set the relevant configuration for both training and testing of the client and its inverted version.\n",
    "\n",
    "> **IMPORTANT: The architecture used here is an MLP because it is affected by inversion of images. Also, note that we must seed the model parameters here.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_ZCNLJRZoah"
   },
   "outputs": [],
   "source": [
    "# Seed the model\n",
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "network_generator_mlp = get_network_generator_mlp()\n",
    "seed_net_mlp = network_generator_mlp()\n",
    "seed_model_mlp_params: NDArrays = get_model_parameters(seed_net_mlp)\n",
    "# Set up config for both train and eval\n",
    "train_config: dict[str, Any] = {\n",
    "    \"epochs\": 8,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "}\n",
    "test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "}\n",
    "# Here is the mapping\n",
    "mapping_dict = {0: \"178\", 1: \"178\"}\n",
    "# NOTE: we are using here the `get_mod_flower_client_generator`\n",
    "federated_mod_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_mod_flower_client_generator(\n",
    "    model_generator=network_generator_mlp,\n",
    "    data_dir=data_dir,\n",
    "    partition_dir=federated_partition,\n",
    "    mapping_fn=lambda x: mapping_dict[x],\n",
    ")\n",
    "# NOTE: we are using here the `get_flower_client_generator`\n",
    "federated_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_flower_client_generator(\n",
    "    model_generator=network_generator_mlp,\n",
    "    data_dir=data_dir,\n",
    "    partition_dir=federated_partition,\n",
    "    mapping_fn=lambda x: mapping_dict[x],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLqrKFqtaIYh"
   },
   "source": [
    "Let's train separately the two versions of the client and then mutually evaluate their models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWYktaFiaH3d",
    "outputId": "11cf6d6e-c24c-4850-f0d4-0541da501e72"
   },
   "outputs": [],
   "source": [
    "# Create clients\n",
    "client_0 = federated_mod_flower_client_generator(0)\n",
    "client_1 = federated_mod_flower_client_generator(1)\n",
    "# Fit client 0\n",
    "client_0_params, *rest = fit_client_seeded(\n",
    "    client_0, seed_model_mlp_params, train_config\n",
    ")\n",
    "log(INFO, f\"Results of fitting the seed model on client 0:\\n\\t{rest}\")\n",
    "# Evaluate client 0 on model trained on client 0\n",
    "client_0_res = client_0.evaluate(client_0_params, test_config)\n",
    "log(\n",
    "    INFO,\n",
    "    \"Results of model eval trained on client 0 on the test set of client 0:\\n\\t%s\",\n",
    "    client_0_res,\n",
    ")\n",
    "# Fit client 1\n",
    "client_1_params, *rest = fit_client_seeded(\n",
    "    client_1, seed_model_mlp_params, train_config\n",
    ")\n",
    "log(INFO, f\"Results of fitting the seed model on client 1:\\n\\t{rest}\")\n",
    "# Evaluate client 1 on model trained on client 1\n",
    "client_1_res = client_1.evaluate(client_1_params, test_config)\n",
    "log(\n",
    "    INFO,\n",
    "    \"Results of model eval trained on client 1 on the test set of client 1:\\n\\t%s\",\n",
    "    client_1_res,\n",
    ")\n",
    "# Evaluate client 0 on model trained on client 0\n",
    "client_0_res = client_0.evaluate(client_1_params, test_config)\n",
    "log(\n",
    "    INFO,\n",
    "    \"Results of model eval trained on client 1 on the test set of client 0:\\n\\t%s\",\n",
    "    client_0_res,\n",
    ")\n",
    "# Evaluate client 1 on model trained on client 1\n",
    "client_1_res = client_1.evaluate(client_0_params, test_config)\n",
    "log(\n",
    "    INFO,\n",
    "    \"Results of model eval trained on client 0 on the test set of client 1:\\n\\t%s\",\n",
    "    client_1_res,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2I9CswibRCb"
   },
   "source": [
    "In the following cell, we will set up all the relevant methods and parameters to run an FL simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pEHYQedk8xW"
   },
   "outputs": [],
   "source": [
    "def _on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    return train_config.update({\"server_round\": server_round})\n",
    "\n",
    "\n",
    "def _on_evaluate_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    return test_config.update({\"server_round\": server_round})\n",
    "\n",
    "\n",
    "# NOTE: We don't need the `federated_evaluation_function`. We care about the\n",
    "# distributed accuracy, thus we skip centralised evaluation.\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=sys.float_info.min,\n",
    "    fraction_evaluate=sys.float_info.min,\n",
    "    min_fit_clients=2,\n",
    "    min_evaluate_clients=2,\n",
    "    min_available_clients=2,\n",
    "    on_fit_config_fn=_on_fit_config_fn,\n",
    "    on_evaluate_config_fn=_on_evaluate_config_fn,\n",
    "    initial_parameters=ndarrays_to_parameters(seed_model_mlp_params),\n",
    "    accept_failures=False,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DZfkIkaYPME"
   },
   "source": [
    "**Question 1 (Part II ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "1. Using the methods implemented so far, set up two FL settings composed of the clients above. One has both clients with their original dataset, and the other has one client with their features inverted. _(Hint: use `federated_flower_client_generator` for the first setting and `federated_mod_flower_client_generator` for the second setting)_\n",
    "2. Train the two settings generated at 1) for 5 rounds using `fl.simulation.start_simulation`. Set `num_clients=2` to use only client IDs in `[0,1]`. Use the `strategy` from the cell above.\n",
    "3. Take note of the evaluation metrics of both FL experiments that you have done. Compare the two using those metrics (plots are recommended). Briefly discuss the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mTgAc5yYVny"
   },
   "source": [
    "### 2.2 Label distribution skew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IE6yFy4sH79L"
   },
   "source": [
    "The flavour of data heterogeneity we are dealing with in this subsection is often called “prior probability shift\". In a few words, this occurs when, given a particular label, the distribution of samples having that label differs from client to client. In our running example, we can see whether in FEMNIST happens that different clients have drawn different distributions of symbols.\n",
    "We can think about what could happen to an FL training in this situation simplifying a bit the actors in a play. Let's imagine that the federation has a subset of clients (one client: client 0) that is the only one having a specific subset of labels (say letters). A global model trained using FedAvg on that federation won't ever be able to learn well the representation of that subset of labels (letters). There exist methods to mitigate this particular situation, but often they involve sharing statistics about local datasets, thus creating privacy concerns.\n",
    "\n",
    "We will select two clients, then we will plot the histogram of the labels for each of them on the same canvas. Thus, we will be able to evaluate whether there is a qualitative difference between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "CobK_i5-cwxI",
    "outputId": "876d5ead-de3b-4012-8e3f-810691e1c944"
   },
   "outputs": [],
   "source": [
    "client_3000_dataset: FEMNIST = FEMNIST(\n",
    "    mapping=federated_partition / \"3000\", data_dir=data_dir, name=\"train\"\n",
    ")\n",
    "plt.hist(\n",
    "    [int(x[1]) for x in client_0_dataset],\n",
    "    bins=62,\n",
    "    color=\"blue\",\n",
    "    alpha=0.7,\n",
    "    label=\"client 0\",\n",
    ")\n",
    "plt.hist(\n",
    "    [int(x[1]) for x in client_3000_dataset],\n",
    "    bins=62,\n",
    "    color=\"orange\",\n",
    "    alpha=0.7,\n",
    "    label=\"client 3000\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ekusyXpeLy3"
   },
   "source": [
    "**Question 2 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n",
    "\n",
    "1. If we ignore the privacy assumption of FL and allow the overall label distribution to be known, how could we balance the set of selected clients to guide the federated model towards a similar per-class accuracy that the centralised model can achieve? Assume FedAvg and the usual client implementation in your reasoning.\n",
    "2. Can you design an automatic data-driven procedure to mitigate label skew? Assume you are operating on a client level, keeping data private, and using standard FedAvg with random client selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR7M2rpHJ5kO"
   },
   "source": [
    "### 2.3 Quantity skew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amXbWzGcIACk"
   },
   "source": [
    "The last kind of heterogeneity we will discuss in detail is the _quantity skew_, which occurs when clients possess different numbers of samples. Last, but not least, it is the most investigated property that real FL datasets have. Also called “unbalancedness\", it has been tackled using many techniques spanning from data manipulation to optimization algorithm design. Even if the literature regarding such techniques is quite broad, relative to FL being a new topic of research, there is still not any agreement about how to deal with unbalancedness. Often different solutions have different accuracy depending on the task and how unbalanced the FL dataset is.\n",
    "\n",
    "Those of you that have well-trained observation skills may recall a plot in the last lab that spoiled this property. Now we'll try to get the global view of FEMNIST data from this perspective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "T-bzX0KvhZ5z",
    "outputId": "c4a7ee39-f56f-4db7-c9e3-ea89c905e7d6"
   },
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    alpha=0.15,\n",
    "    legend=True,\n",
    "    data=[\n",
    "        len(\n",
    "            FEMNIST(\n",
    "                mapping=federated_partition / str(i), data_dir=data_dir, name=\"train\"\n",
    "            )\n",
    "        )\n",
    "        for i in range(3229)\n",
    "    ],\n",
    "    fill=True,\n",
    "    kde=True,\n",
    "    element=\"step\",\n",
    "    stat=\"density\",\n",
    "    common_norm=False,\n",
    "    common_bins=True,\n",
    "    cbar=True,\n",
    "    palette=\"dark\",\n",
    ")\n",
    "plt.xlabel(\"# samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCkfjXY1hX7D"
   },
   "source": [
    "**Question 3 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n",
    "\n",
    "1. Given the skewness of number of samples held by each client plotted above, list one potential advantage and one potential disadvantage of training only clients with a number of samples close to the mean (e.g., within one $\\sigma$ of the mean).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFYZaMvAcaYV"
   },
   "source": [
    "# 3. The behaviour of FL under data heterogeneity/creating heterogeneous partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zemO_m5s2Bff"
   },
   "source": [
    "## FEMNIST folder structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsTGyV4ms_bm"
   },
   "source": [
    "By default, FEMNIST will be split as follows:\n",
    "\n",
    "- `femnist`: the location of the relevant data\n",
    "  - `client_data_mappings`: contains different partitions\n",
    "    - `centralized`: mappings from writer_id -> `[image_path, label]` for the centralized dataset\n",
    "    - `fed_natural`: mappings from writer_id -> `[image_path, label]` for the naturally federated dataset\n",
    "  - `data`: contains the macro partitions between test, train, and val\n",
    "    - `train`: contains the raw images---samples of the train set\n",
    "    - `test`: contains the raw images---samples of the test set\n",
    "    - `val`: contains the raw images---samples of the val set\n",
    "- `femnist.tar.gz`: the compressed dataset\n",
    "\n",
    "We will store client datasets in sequentially labelled folders within each partition and do all necessary remapping in python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZjU-2dyrVxC",
    "outputId": "ff22ac9e-73f4-476c-caf6-dfca749f0154"
   },
   "outputs": [],
   "source": [
    "# Showing resulting folder tree\n",
    "! tree -dC -L 3 ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_QqLvPZ2Bff"
   },
   "source": [
    "## Test sets for FL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhsBuoiOtDzE"
   },
   "source": [
    "Prior to constructing our centralized partition, it is necessary to discuss what a test set even means for FL.\n",
    "\n",
    "A testing set can generally be either a **federated test set** or a **local test set**:\n",
    "\n",
    "- A federated test set contains data representative of the entire federated network and is used to provide a form of centralized-like evaluation of the federated model. In production scenarios, this would be data that has been consensually gathered from a multitude of users and is legal/efficient to store on the server or clients which have been kept out of the training loop. For research purposes it can be constructed in one of the two following ways:\n",
    "  - Take x% of data from all clients and save it separately.\n",
    "  - Leave y% of clients utterly unavailable for federated training and use their data as a test/validation set. **This is the version we shall use during the lab for the federated test set.**\n",
    "- A local test set is formed via data from a specific client which has not been seen during training. There are as many local test sets as there are clients. As such, they can be used to test the model on a specific client or to accumulate average statistics to determine its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEnNRUzntSHQ"
   },
   "source": [
    "## Creating and using partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XBPXfzPtkJe"
   },
   "source": [
    "### Class unbalancedness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEzJSW4TvEmK"
   },
   "source": [
    "We will start creating class unbalanced partitions. The most representative example, as often happens, is the extreme one. Thus, we are going to create a partition of the dataset in which each client possesses one class only. This aim sets one parameter for the creation of the current partition, but we need to choose also another parameter: the number of clients we want to create or the number of samples per client that we want to infer. These last are two antagonist parameters that describe the same thing. The larger the number of clients is, the lower the number of samples per client is.\n",
    "\n",
    "Let's start by looking at how labels are distributed in the entire dataset (test set only), by treating the dataset as centralized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "yLEQJLkjwM4u",
    "outputId": "704b99b0-a234-4327-e692-43531e1ce524"
   },
   "outputs": [],
   "source": [
    "centralized_train_dataset: FEMNIST = FEMNIST(\n",
    "    mapping=centralized_partition / \"0\", data_dir=data_dir, name=\"train\"\n",
    ")\n",
    "centralized_test_dataset: FEMNIST = FEMNIST(\n",
    "    mapping=centralized_partition / \"0\", data_dir=data_dir, name=\"test\"\n",
    ")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "train_histo = ax[0].hist(\n",
    "    [int(x[1]) for x in centralized_train_dataset.data],\n",
    "    bins=62,\n",
    "    color=\"blue\",\n",
    "    alpha=0.7,\n",
    "    label=\"centralized train dataset\",\n",
    ")\n",
    "test_histo = ax[1].hist(\n",
    "    [int(x[1]) for x in centralized_test_dataset.data],\n",
    "    bins=62,\n",
    "    color=\"orange\",\n",
    "    alpha=0.7,\n",
    "    label=\"centralized test dataset\",\n",
    ")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AWfsLZo9jmi"
   },
   "source": [
    "Since the distribution is all but uniform, we need to make a further choice. We want to build a federation of 62 clients each of them having one class only. We want all the classes to be represented in the federation. Likewise, we also want FedAvg to treat equally all the clients in the federation. Thus, we will need to partition clients taking into account the population of the least represented class in both the `train` and the `test` set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_yGo1zI9UyA",
    "outputId": "dc57594b-63b0-46e5-e59b-3902ac8e29f8"
   },
   "outputs": [],
   "source": [
    "log(\n",
    "    INFO,\n",
    "    \"The least represented class in the train set has %s samples\",\n",
    "    min(train_histo[0]),\n",
    ")\n",
    "log(\n",
    "    INFO,\n",
    "    \"The least represented class in the test set has %s samples\",\n",
    "    min(test_histo[0]),\n",
    ")\n",
    "max_train_samples = int(min(train_histo[0]))\n",
    "max_test_samples = int(min(test_histo[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuvZvcU3_eq0"
   },
   "source": [
    "Let's now create the function that creates the partition. We will stick to the folder structure the dataset has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvJS4NX5_r-F"
   },
   "outputs": [],
   "source": [
    "class_unbalanced_partition: Path = (\n",
    "    dataset_dir / \"client_data_mappings\" / \"class_unbalanced\"\n",
    ")\n",
    "class_unbalanced_partition.mkdir(parents=True, exist_ok=True)\n",
    "train_df = pd.read_csv(centralized_mapping / \"train.csv\")\n",
    "test_df = pd.read_csv(centralized_mapping / \"test.csv\")\n",
    "max_train_samples = 200\n",
    "max_test_samples = 50\n",
    "for i in range(62):\n",
    "    folder_path: Path = class_unbalanced_partition / str(i)\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_path: Path = folder_path / \"train.csv\"\n",
    "    test_path: Path = folder_path / \"test.csv\"\n",
    "\n",
    "    client_df = deepcopy(\n",
    "        train_df[train_df.label == i].sample(frac=1)[:max_train_samples]\n",
    "    ).reset_index()\n",
    "    client_df[\"client_id\"] = i\n",
    "    client_df.drop(columns=[\"level_0\"], inplace=True)\n",
    "    client_df.to_csv(train_path, index=False)\n",
    "    client_df = deepcopy(\n",
    "        test_df[test_df.label == i].sample(frac=1)[:max_test_samples]\n",
    "    ).reset_index()\n",
    "    client_df[\"client_id\"] = i\n",
    "    client_df.drop(columns=[\"level_0\"], inplace=True)\n",
    "    client_df.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we seed the initial model parameters to come from a partially trained model on the centralized dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0Bj3btM9llx"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "network_generator_cnn = get_network_generator_cnn()\n",
    "seed_net_cnn = network_generator_cnn()\n",
    "centralized_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_flower_client_generator(\n",
    "    model_generator=network_generator_cnn,\n",
    "    partition_dir=centralized_partition,\n",
    "    data_dir=data_dir,\n",
    ")\n",
    "centralized_flower_client = centralized_flower_client_generator(0)\n",
    "centralized_train_config: dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": 1000,\n",
    "}\n",
    "test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "}\n",
    "# Train parameters on the centralised dataset\n",
    "trained_params, num_examples, train_metrics = fit_client_seeded(\n",
    "    centralized_flower_client,\n",
    "    params=get_model_parameters(seed_net_cnn),\n",
    "    conf=centralized_train_config,\n",
    ")\n",
    "initial_parameters: Parameters = ndarrays_to_parameters(trained_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create the centralised evaluation function that will be executed by the server at the end of every round if requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRiXe-eU9m4u"
   },
   "outputs": [],
   "source": [
    "federated_evaluation_function = get_federated_evaluation_function(\n",
    "    data_dir=data_dir,\n",
    "    centralized_mapping=centralized_mapping,\n",
    "    device=get_device(),\n",
    "    batch_size=test_config[\"batch_size\"],\n",
    "    num_workers=test_config[\"num_workers\"],\n",
    "    model_generator=network_generator_cnn,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the client generator function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0Ohncc3DLJG"
   },
   "outputs": [],
   "source": [
    "unbalanced_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_flower_client_generator(\n",
    "    model_generator=network_generator_cnn,\n",
    "    data_dir=data_dir,\n",
    "    partition_dir=class_unbalanced_partition,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these clients look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 62\n",
    "plt.hist(\n",
    "    [\n",
    "        int(x[1])\n",
    "        for x in unbalanced_flower_client_generator(0)._load_dataset(\"train\").data\n",
    "    ],\n",
    "    bins=N_CLASSES,\n",
    "    color=\"blue\",\n",
    "    alpha=0.7,\n",
    "    label=\"client 0 train set\",\n",
    ")\n",
    "plt.hist(\n",
    "    [\n",
    "        int(x[1])\n",
    "        for x in unbalanced_flower_client_generator(1)._load_dataset(\"train\").data\n",
    "    ],\n",
    "    bins=N_CLASSES,\n",
    "    color=\"orange\",\n",
    "    alpha=0.7,\n",
    "    label=\"client 1 train set\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a wrapper for the strategy that will be used to extract the clients' model parameters obtained during the training for the next question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import FitRes, parameters_to_ndarrays\n",
    "\n",
    "\n",
    "class WrappedFedAvg(FedAvg):\n",
    "    clients_models: dict[int, list[tuple[int, NDArrays]]] = {}\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: list[tuple[ClientProxy, FitRes]],\n",
    "        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n",
    "    ) -> tuple[Parameters | None, dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "        # Call FedAvg original aggregate_fit, so that it handles the failures\n",
    "        ret = super().aggregate_fit(server_round, results, failures)\n",
    "        # Append clients' model parameters to the list\n",
    "        self.clients_models[server_round] = [\n",
    "            (i, parameters_to_ndarrays(fit_res.parameters))\n",
    "            for i, (_, fit_res) in enumerate(results)\n",
    "        ]\n",
    "        # Return the original return value\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuSAFGmXDgYT"
   },
   "outputs": [],
   "source": [
    "train_config: dict[str, Any] = {\n",
    "    \"epochs\": 8,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    return train_config | {\"server_round\": server_round}\n",
    "\n",
    "\n",
    "num_total_clients = 62\n",
    "num_clients_per_round: int = 5\n",
    "num_evaluate_clients: int = 5\n",
    "\n",
    "strategy = WrappedFedAvg(\n",
    "    fraction_fit=sys.float_info.min,\n",
    "    fraction_evaluate=sys.float_info.min,\n",
    "    min_fit_clients=num_clients_per_round,\n",
    "    min_evaluate_clients=num_evaluate_clients,\n",
    "    min_available_clients=max(num_clients_per_round, num_evaluate_clients),\n",
    "    on_fit_config_fn=_on_fit_config_fn,\n",
    "    on_evaluate_config_fn=None,\n",
    "    evaluate_fn=federated_evaluation_function,\n",
    "    initial_parameters=initial_parameters,\n",
    "    accept_failures=False,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5sFPqF0k596b",
    "outputId": "3c8b36f3-1cf7-448f-a60e-36d2d1c18043"
   },
   "outputs": [],
   "source": [
    "params, hist = start_seeded_simulation(\n",
    "    client_fn=lambda cid: unbalanced_flower_client_generator(cid).to_client(),\n",
    "    num_clients=num_total_clients,\n",
    "    config=ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    "    name=\"unbalanced\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now investigate how such a partition behaves in an FL setting. In particular, we will try to understand what happens to the global model and the clients' updates as the training progresses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzG9xF-lgb51"
   },
   "source": [
    "**Question 4 (Part II ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "1. For each round, retrieve the clients' models and the global model using the appropriate attribute of the `WrappedFedAvg` strategy.\n",
    "2. Inspect the models collected for each round as follows:\n",
    "   - Extract the \"softmax-ed\" activations of the last layer of each clients' model when the model is fed with random inputs. (HINT: `from common.client_utils import get_activations_from_random_input`, give a motivation for the choice of the parameters of this function if you decide to use it)\n",
    "   - Compute the pairwise cosine-similarity (you can use functions similar to those used in the previous Lab) between the values obtained in the previous step.\n",
    "   - Plot the results of this computation in a confusion matrix. The confusion matrix will have the shape `n_clients_per_round`x`n_clients_per_round`.\n",
    "   - Repeat the same procedure for computing and compare the pairwise KL divergence between the \"softmax-ed\" activations of the last layer of each clients' model when the model is fed with random inputs. (HINT: `from scipy.stats import entropy`).\n",
    "3. Compare the confusion matrices obtained. What do you observe, and how does it compare with your expectations? Briefly discuss the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (Part III/MPhil ✅):**\n",
    "\n",
    "(This is meant to be a conceptual question. You should provide a written answer to this. **No more than 3 sentences**. **No code** is needed)\n",
    "\n",
    "1. How does the concept of a client relate to a task in Multi-task Learning (MTL)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I64eyic-tnWF"
   },
   "source": [
    "### LDA partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLFg0zOBh77Q"
   },
   "source": [
    "The most popular method for creating heterogeneous partitions from a centralized dataset in terms of class unbalancing is [Latent Dirichlet Allocation (LDA)](https://web.archive.org/web/20120501152722/http://jmlr.csail.mit.edu/papers/v3/blei03a.html). LDA is a generative probabilistic model for collections of discrete data. The paper linked above provides all the theoretical details about the method.\n",
    "\n",
    "The important detail for you to understand about LDA is that the `concentration` parameter controls the degree of heterogeneity in the distribution while `num_partitions` controls how many clients are generated following the specified distribution.\n",
    "\n",
    "A `concentration=0` implies a completely heterogeneous distribution where each client may only contain examples from one class. Thus, if for a `concentration=0` we were to set the `num_partitions` argument to the number of classes we will get the same partitioning as the one we manually created above.\n",
    "\n",
    "> **Important:** The LDA partitioning only approaches a fully i.i.d distribution when `concentration` tends towards $\\infty$. The most appropriate value of the `concentration` to generate a completely i.i.d. partition is dataset dependent. Feel free to try out different values of `concentration` and `num_partitions` to see how the partitioning changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNho5BDtlpbh"
   },
   "outputs": [],
   "source": [
    "from common.lda_utils import create_lda_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the partitions using LDA with `concentration=2.5` and `num_partitions=1000`. This will result in a non-i.i.d. partitioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztmKkBPLhBVP"
   },
   "outputs": [],
   "source": [
    "N_TOTAL_CLIENTS = 1_000\n",
    "concentration = 2.5\n",
    "# Create partitions\n",
    "x = np.array([x[0] for x in centralized_train_dataset.data])\n",
    "y = np.array([x[1] for x in centralized_train_dataset.data])\n",
    "train_clients_partitions, dist = create_lda_partitions(\n",
    "    dataset=(x, y),\n",
    "    dirichlet_dist=None,\n",
    "    num_partitions=N_TOTAL_CLIENTS,\n",
    "    concentration=concentration,\n",
    "    accept_imbalanced=True,\n",
    "    seed=Seeds.DEFAULT,\n",
    ")\n",
    "x = np.array([x[0] for x in centralized_test_dataset.data])\n",
    "y = np.array([x[1] for x in centralized_test_dataset.data])\n",
    "test_clients_partitions, dist = create_lda_partitions(\n",
    "    dataset=(x, y),\n",
    "    dirichlet_dist=dist,\n",
    "    num_partitions=N_TOTAL_CLIENTS,\n",
    "    concentration=concentration,\n",
    "    accept_imbalanced=True,\n",
    "    seed=Seeds.DEFAULT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the partitions in the folder structure we have been using so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EFfE0fVqnCB"
   },
   "outputs": [],
   "source": [
    "lda_partition: Path = dataset_dir / \"client_data_mappings\" / \"lda\"\n",
    "if lda_partition.exists():\n",
    "    ! rm -rf {str(lda_partition)}\n",
    "lda_partition.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(\n",
    "    zip(train_clients_partitions, test_clients_partitions, strict=True)\n",
    "):\n",
    "    folder_path: Path = lda_partition / str(i)\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_path: Path = folder_path / \"train.csv\"\n",
    "    test_path: Path = folder_path / \"test.csv\"\n",
    "\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"client_id\": [0] * len(train_set[0]),\n",
    "            \"sample_path\": train_set[0],\n",
    "            \"sample_id\": range(len(train_set[0])),\n",
    "            \"label\": train_set[1],\n",
    "        }\n",
    "    ).to_csv(train_path, index=False, mode=\"w\")\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"client_id\": [0] * len(test_set[0]),\n",
    "            \"sample_path\": test_set[0],\n",
    "            \"sample_id\": range(len(test_set[0])),\n",
    "            \"label\": test_set[1],\n",
    "        }\n",
    "    ).to_csv(test_path, index=False, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a client generator function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm6IBCqGqgHD"
   },
   "outputs": [],
   "source": [
    "lda_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_flower_client_generator(\n",
    "    model_generator=network_generator_cnn,\n",
    "    data_dir=data_dir,\n",
    "    partition_dir=lda_partition,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot two clients' labels distributions to see how different they are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGf3J7SytbIc"
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 62\n",
    "plt.hist(\n",
    "    [int(x[1]) for x in lda_flower_client_generator(0)._load_dataset(\"train\").data],\n",
    "    bins=N_CLASSES,\n",
    "    color=\"blue\",\n",
    "    alpha=0.7,\n",
    "    label=\"client 0 train set\",\n",
    ")\n",
    "plt.hist(\n",
    "    [int(x[1]) for x in lda_flower_client_generator(1)._load_dataset(\"train\").data],\n",
    "    bins=N_CLASSES,\n",
    "    color=\"orange\",\n",
    "    alpha=0.7,\n",
    "    label=\"client 1 train set\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try now to run an FL simulation is such challenging setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWbg6Ty6s7c0"
   },
   "outputs": [],
   "source": [
    "train_config: dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    return train_config | {\"server_round\": server_round}\n",
    "\n",
    "\n",
    "num_total_clients = N_TOTAL_CLIENTS\n",
    "num_clients_per_round: int = 5\n",
    "num_evaluate_clients: int = 0\n",
    "\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=sys.float_info.min,\n",
    "    fraction_evaluate=sys.float_info.min,\n",
    "    min_fit_clients=num_clients_per_round,\n",
    "    min_evaluate_clients=num_evaluate_clients,\n",
    "    min_available_clients=max(num_clients_per_round, num_evaluate_clients),\n",
    "    on_fit_config_fn=_on_fit_config_fn,\n",
    "    on_evaluate_config_fn=None,\n",
    "    evaluate_fn=federated_evaluation_function,\n",
    "    initial_parameters=initial_parameters,\n",
    "    accept_failures=False,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PscXeuI6Dpo"
   },
   "outputs": [],
   "source": [
    "params, hist = start_seeded_simulation(\n",
    "    client_fn=lambda cid: lda_flower_client_generator(cid).to_client(),\n",
    "    num_clients=num_total_clients,\n",
    "    config=ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    "    name=\"fedavg_lda_example\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (Part II ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "1. Produce a set of LDA partitions with `concentration` in `[0.001, 0.1, 1e3]`.\n",
    "2. Train an FL setting for every partition you have produced using the same hyperparameter we used in the cell above, but with `num_clients_per_round=62` and `ServerConfig(num_rounds=1)`.\n",
    "3. Why do you think one-shot averaging works well for full i.i.d data and not for very heterogeneous clients?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the configuration provided by the following cell.\n",
    "\n",
    "> **IMPORTANT**: Be careful when using the strategy object. You must ensure that the different experiments will use the same initial parameters. Inspect `flwr.server.strategy.FedAvg` to understand how these are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We are using randomly initialized parameters here.\n",
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "network_generator_cnn = get_network_generator_cnn()\n",
    "seed_net_cnn = network_generator_cnn()\n",
    "q6_initial_parameters: Parameters = ndarrays_to_parameters(\n",
    "    get_model_parameters(seed_net_cnn)\n",
    ")\n",
    "# Set up experiment configuration\n",
    "N_TOTAL_CLIENTS = 100\n",
    "N_TOTAL_ROUNDS = 1\n",
    "train_config: dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.001,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    return train_config | {\"server_round\": server_round}\n",
    "\n",
    "\n",
    "num_total_clients = N_TOTAL_CLIENTS\n",
    "num_clients_per_round: int = 62\n",
    "num_evaluate_clients: int = 0\n",
    "fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
    "fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
    "# Set up strategy\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=sys.float_info.min,\n",
    "    fraction_evaluate=sys.float_info.min,\n",
    "    min_fit_clients=num_clients_per_round,\n",
    "    min_evaluate_clients=num_evaluate_clients,\n",
    "    min_available_clients=max(num_clients_per_round, num_evaluate_clients),\n",
    "    on_fit_config_fn=_on_fit_config_fn,\n",
    "    on_evaluate_config_fn=None,\n",
    "    evaluate_fn=federated_evaluation_function,\n",
    "    initial_parameters=q6_initial_parameters,\n",
    "    accept_failures=False,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0_aN4dkcvv8"
   },
   "source": [
    "# 4. FL Strategies tackling heterogeneity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ayxhgBa-8Oi"
   },
   "source": [
    "The challenge of dealing with non-iidness in FL has been discussed extensively since the publication of the first paper about FL. Many approaches have been proposed, but still, the literature doesn't agree on a general approach or a rule of thumb. A natural method is to modify/extend FedAvg or develop another algorithm specifically oriented to mitigating heterogeneity. In some applications, augmenting data to make it more similar between clients is possible. Some works assume a small dataset that is shared between clients to serve as a reference.\n",
    "\n",
    "Every time a new approach has been proposed, many related questions have arisen. It is no longer clear that treating all examples of all the clients equally make sense. Some works proposed limiting the contributions of data from any client in the federation. The notion of fairness has been introduced and defined in many ways in order to set up principles that could result in new approaches, for example emphasizing underperforming clients during aggregation. It is not even clear whether a single global model is the correct objective for FL---works related to this question gave birth to Personalised Federated Learning (PFL), a sub-branch of research.\n",
    "\n",
    "We started to think that we should be able to turn the non-iid problem from a bug into a feature treated similarly to a task in MTL. The number of works that start with this perspective is however limited.\n",
    "\n",
    "Even if FedAvg can partially mitigate heterogeneity, it is not working well in all situations, as we saw previously. From the many new algorithms that were introduced, it is worth mentioning: [FedProx](https://www.researchgate.net/profile/Anit-Sahu/publication/329734586_On_the_Convergence_of_Federated_Optimization_in_Heterogeneous_Networks/links/5c1bdd5e299bf12be38ee52d/On-the-Convergence-of-Federated-Optimization-in-Heterogeneous-Networks.pdf), [q-FedAvg](https://arxiv.org/abs/1905.10497), [SCAFFOLD](http://proceedings.mlr.press/v119/karimireddy20a.html). For a matter of time, we will now explore just one of these algorithms introduced for tackling the non-iid problem specifically. We chose FedProx since it is the simplest to demonstrate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3xe9fHHc5Sc"
   },
   "source": [
    "## FedProx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNyYEsRnId3C"
   },
   "source": [
    "The Federated Proximal (FedProx) method was developed in the context of tuning and benchmarking FedAvg in a heterogeneous setting. The authors of FedAvg proposed carefully tuning the learning rate and the number of local epochs to increase the accuracy in such settings but turned out clear that something more was necessary. Based on the client's heterogeneity, local updates change the global model not only in different directions in the loss space but also at different rates. This phenomenon is called client divergence or drifting.\n",
    "\n",
    "Using FedAvg we would like to have a different number of local epochs for each client based on its characteristics. Thus, heuristically setting the number of local updates is not always optimal, because clients are all different from each other. Limiting the number of local updates through a more flexible tool is beneficial. Thus, FedProx proposes to incorporate a term in the local objective function that penalizes big changes from the current model at the server. In each node $k$, instead of minimizing the local loss function $F_k$, the local solver tries to approximately minimize:\n",
    "\n",
    "$\\min_wh_w(w;w^t)=F_k(w)+\\frac{\\mu}{2}||w-w^t||^2$,\n",
    "\n",
    "where $||w-w^t||^2$ is the new proximal term.\n",
    "\n",
    "FedProx acts as FedAvg apart from the change in the local objective. This new objective enforces limited local model updates more explicitly than FedAvg. Using FedProx, it is no longer necessary to tune the number of local epochs for each client to minimize divergence.\n",
    "\n",
    "We will now implement and test FedProx on the natural partition of FEMNIST. Implementing FedProx just involves modifying the local training function of our `FlowerRayClient`. We will then write the new function and build a new `client_generator` function that overwrites the relevant methods of our client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoGXFQALRRkR"
   },
   "outputs": [],
   "source": [
    "def train_fedprox_FEMNIST(\n",
    "    net: Module,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int,\n",
    "    device: str,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: Module,\n",
    "    proximal_mu: float,\n",
    ") -> float:\n",
    "    \"\"\"Trains the network on the training set using FedProx.\n",
    "\n",
    "    Args:\n",
    "        net (Module): generic module object describing the network to train.\n",
    "        train_loader (DataLoader): dataloader to iterate during the training.\n",
    "        epochs (int): number of epochs of training.\n",
    "        device (str): device name onto which perform the computation.\n",
    "        optimizer (torch.optim.Optimizer): optimizer object.\n",
    "        criterion (Module): generic module describing the loss function.\n",
    "        proximal_mu (float): parameter for the weight of the proximal term.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        float: the final epoch mean train loss.\n",
    "    \"\"\"\n",
    "    global_params: Module = deepcopy(net)\n",
    "    global_params.requires_grad_(False)\n",
    "    global_params = [val for _, val in global_params.state_dict().items()]\n",
    "    net.train()\n",
    "    running_loss, total = 0.0, 0\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # NOTE: here comes the implementation of FedProx algorithm\n",
    "            proximal_term = 0.0\n",
    "            for local_weights, global_weights in zip(\n",
    "                net.parameters(), global_params, strict=True\n",
    "            ):\n",
    "                proximal_term += (local_weights - global_weights).norm(2)\n",
    "            loss = criterion(net(data), labels) + (proximal_mu / 2) * proximal_term\n",
    "            running_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return running_loss / total\n",
    "\n",
    "\n",
    "def _train_fedprox(\n",
    "    self, net: Module, train_loader: DataLoader, config: dict[str, Scalar]\n",
    ") -> float:\n",
    "    return train_fedprox_FEMNIST(\n",
    "        net=net,\n",
    "        train_loader=train_loader,\n",
    "        epochs=int(config[\"epochs\"]),\n",
    "        device=self.device,\n",
    "        optimizer=torch.optim.AdamW(\n",
    "            net.parameters(),\n",
    "            lr=float(config[\"client_learning_rate\"]),\n",
    "            weight_decay=float(config[\"weight_decay\"]),\n",
    "        ),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        proximal_mu=config[\"proximal_mu\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_fedprox_flower_client_generator(\n",
    "    model_generator: Callable[[], Module],\n",
    "    data_dir: Path,\n",
    "    partition_dir: Path,\n",
    ") -> Callable[[str], FlowerClient]:\n",
    "    \"\"\"Implement a wrapper function for the client instance generator.\n",
    "\n",
    "    This provides the client generator with a model generator function.\n",
    "    Also, the partition directory must be passed.\n",
    "    The clients generated will train using FedProx algorithm.\n",
    "\n",
    "    Args:\n",
    "        data_dir (Path): path to the dataset folder.\n",
    "        model_generator (Callable[[], Module]): model generator function.\n",
    "        partition_dir (Path): directory containing the partition.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Callable[[str], WrappedClient]: client instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> FlowerClient:\n",
    "        \"\"\"Create a single client instance given the client id `cid`.\n",
    "\n",
    "        Args:\n",
    "            cid (str): client id, Flower requires this to of type str.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            WrappedClient: client instance.\n",
    "        \"\"\"\n",
    "        log(INFO, f\"Getting client with id {cid}\")\n",
    "        client = FlowerClient(\n",
    "            cid=cid,\n",
    "            data_dir=data_dir,\n",
    "            partition_dir=partition_dir,\n",
    "            model_generator=model_generator,\n",
    "        )\n",
    "        # Pay attention to the following line\n",
    "        client._train = _train_fedprox.__get__(client, FlowerClient)\n",
    "        return client\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "N_TOTAL_CLIENTS = 1000\n",
    "CONCENTRATION = 0.1\n",
    "# Create partitions\n",
    "x = np.array([x[0] for x in centralized_train_dataset.data])\n",
    "y = np.array([x[1] for x in centralized_train_dataset.data])\n",
    "train_clients_partitions, dist = create_lda_partitions(\n",
    "    dataset=(x, y),\n",
    "    dirichlet_dist=None,\n",
    "    num_partitions=N_TOTAL_CLIENTS,\n",
    "    concentration=CONCENTRATION,\n",
    "    accept_imbalanced=True,\n",
    "    seed=Seeds.DEFAULT,\n",
    ")\n",
    "x = np.array([x[0] for x in centralized_test_dataset.data])\n",
    "y = np.array([x[1] for x in centralized_test_dataset.data])\n",
    "test_clients_partitions, dist = create_lda_partitions(\n",
    "    dataset=(x, y),\n",
    "    dirichlet_dist=dist,\n",
    "    num_partitions=N_TOTAL_CLIENTS,\n",
    "    concentration=CONCENTRATION,\n",
    "    accept_imbalanced=True,\n",
    ")\n",
    "# Store partitions\n",
    "lda_partition: Path = dataset_dir / \"client_data_mappings\" / f\"lda_{CONCENTRATION}\"\n",
    "if lda_partition.exists():\n",
    "    ! rm -rf {str(lda_partition)}\n",
    "lda_partition.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(\n",
    "    zip(train_clients_partitions, test_clients_partitions, strict=True)\n",
    "):\n",
    "    folder_path: Path = lda_partition / str(i)\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_path: Path = folder_path / \"train.csv\"\n",
    "    test_path: Path = folder_path / \"test.csv\"\n",
    "\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"client_id\": [0] * len(train_set[0]),\n",
    "            \"sample_path\": train_set[0],\n",
    "            \"sample_id\": range(len(train_set[0])),\n",
    "            \"label\": train_set[1],\n",
    "        }\n",
    "    ).to_csv(train_path, index=False)\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"client_id\": [0] * len(test_set[0]),\n",
    "            \"sample_path\": test_set[0],\n",
    "            \"sample_id\": range(len(test_set[0])),\n",
    "            \"label\": test_set[1],\n",
    "        }\n",
    "    ).to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjiLOOddl6cw"
   },
   "source": [
    "We are now able to train an FL setting using FedProx.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sbd8Ks4Hl-Uz"
   },
   "outputs": [],
   "source": [
    "# NOTE: we are using here the `get_fedprox_flower_client_generator`\n",
    "federated_fedprox_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_fedprox_flower_client_generator(\n",
    "    model_generator=network_generator_cnn,\n",
    "    data_dir=data_dir,\n",
    "    partition_dir=lda_partition,\n",
    ")\n",
    "N_TOTAL_ROUNDS = 5\n",
    "# Set up experiment configuration\n",
    "train_config: dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.1,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "    \"proximal_mu\": 0.01,\n",
    "}\n",
    "\n",
    "\n",
    "def _on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    return train_config | {\"server_round\": server_round}\n",
    "\n",
    "\n",
    "num_total_clients = N_TOTAL_CLIENTS\n",
    "num_clients_per_round: int = 5\n",
    "num_evaluate_clients: int = 0\n",
    "fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
    "fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
    "# Set up strategy\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=sys.float_info.min,\n",
    "    fraction_evaluate=sys.float_info.min,\n",
    "    min_fit_clients=num_clients_per_round,\n",
    "    min_evaluate_clients=num_evaluate_clients,\n",
    "    min_available_clients=max(num_clients_per_round, num_evaluate_clients),\n",
    "    on_fit_config_fn=_on_fit_config_fn,\n",
    "    on_evaluate_config_fn=None,\n",
    "    evaluate_fn=federated_evaluation_function,\n",
    "    initial_parameters=initial_parameters,\n",
    "    accept_failures=False,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cg576GQBRWS"
   },
   "outputs": [],
   "source": [
    "params_list, hist = start_seeded_simulation(\n",
    "    client_fn=lambda cid: federated_fedprox_flower_client_generator(cid).to_client(),\n",
    "    num_clients=N_TOTAL_CLIENTS,\n",
    "    config=ServerConfig(num_rounds=N_TOTAL_ROUNDS),\n",
    "    strategy=strategy,\n",
    "    name=\"fedprox\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (Part II ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "1. Train the FL setting composed of LDA partitions with `concentration=1.0` using FedProx with values of `proximal_mu` in $\\{10.0, 0.1, 1e-5\\}$. Keep `num_clients_per_round=5` and `ServerConfig(num_rounds=10)`. Use the experimental configurations bellow.\n",
    "2. Plot convergence curves for both `proximal_mu`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "network_generator_cnn = get_network_generator_cnn()\n",
    "seed_net_cnn = network_generator_cnn()\n",
    "centralized_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_flower_client_generator(\n",
    "    model_generator=network_generator_cnn,\n",
    "    partition_dir=centralized_partition,\n",
    "    data_dir=data_dir,\n",
    ")\n",
    "centralized_flower_client = centralized_flower_client_generator(0)\n",
    "centralized_train_config: dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": 1000,\n",
    "}\n",
    "# Train parameters on the centralised dataset\n",
    "trained_params, num_examples, train_metrics = fit_client_seeded(\n",
    "    centralized_flower_client,\n",
    "    params=get_model_parameters(seed_net_cnn),\n",
    "    conf=centralized_train_config,\n",
    ")\n",
    "q7_initial_parameters_pretrained = ndarrays_to_parameters(trained_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOTAL_CLIENTS = 1000\n",
    "N_TOTAL_ROUNDS = 10\n",
    "N_CLIENTS_PER_ROUND = 5\n",
    "CONCENTRATION = 1.0\n",
    "# Set up experiment configuration\n",
    "train_config: dict[str, Any] = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 2,\n",
    "    \"max_batches\": None,\n",
    "    # NOTE: This must be adjusted according to the requests\n",
    "    \"proximal_mu\": ...,\n",
    "}\n",
    "\n",
    "\n",
    "def _on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    return train_config | {\"server_round\": server_round}\n",
    "\n",
    "\n",
    "num_total_clients = N_TOTAL_CLIENTS\n",
    "num_clients_per_round: int = N_CLIENTS_PER_ROUND\n",
    "num_evaluate_clients: int = 0\n",
    "fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
    "fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
    "# Set up strategy\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=sys.float_info.min,\n",
    "    fraction_evaluate=sys.float_info.min,\n",
    "    min_fit_clients=num_clients_per_round,\n",
    "    min_evaluate_clients=num_evaluate_clients,\n",
    "    min_available_clients=max(num_clients_per_round, num_evaluate_clients),\n",
    "    on_fit_config_fn=_on_fit_config_fn,\n",
    "    on_evaluate_config_fn=None,\n",
    "    evaluate_fn=federated_evaluation_function,\n",
    "    initial_parameters=q7_initial_parameters_pretrained,\n",
    "    accept_failures=False,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    ")\n",
    "\n",
    "# Create partitions\n",
    "x = np.array([x[0] for x in centralized_train_dataset.data])\n",
    "y = np.array([x[1] for x in centralized_train_dataset.data])\n",
    "train_clients_partitions, dist = create_lda_partitions(\n",
    "    dataset=(x, y),\n",
    "    dirichlet_dist=None,\n",
    "    num_partitions=N_TOTAL_CLIENTS,\n",
    "    concentration=CONCENTRATION,\n",
    "    accept_imbalanced=True,\n",
    ")\n",
    "x = np.array([x[0] for x in centralized_test_dataset.data])\n",
    "y = np.array([x[1] for x in centralized_test_dataset.data])\n",
    "test_clients_partitions, dist = create_lda_partitions(\n",
    "    dataset=(x, y),\n",
    "    dirichlet_dist=dist,\n",
    "    num_partitions=N_TOTAL_CLIENTS,\n",
    "    concentration=CONCENTRATION,\n",
    "    accept_imbalanced=True,\n",
    ")\n",
    "# Store partitions\n",
    "lda_partition: Path = dataset_dir / \"client_data_mappings\" / f\"lda_{CONCENTRATION}\"\n",
    "if lda_partition.exists():\n",
    "    ! rm -rf {lda_partition}\n",
    "lda_partition.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(\n",
    "    zip(train_clients_partitions, test_clients_partitions, strict=True)\n",
    "):\n",
    "    folder_path: Path = lda_partition / str(i)\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_path: Path = folder_path / \"train.csv\"\n",
    "    test_path: Path = folder_path / \"test.csv\"\n",
    "\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"client_id\": [0] * len(train_set[0]),\n",
    "            \"sample_path\": train_set[0],\n",
    "            \"sample_id\": range(len(train_set[0])),\n",
    "            \"label\": train_set[1],\n",
    "        }\n",
    "    ).to_csv(train_path, index=False)\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"client_id\": [0] * len(test_set[0]),\n",
    "            \"sample_path\": test_set[0],\n",
    "            \"sample_id\": range(len(test_set[0])),\n",
    "            \"label\": test_set[1],\n",
    "        }\n",
    "    ).to_csv(test_path, index=False)\n",
    "# Create the client generator\n",
    "fedprox_flower_client_generator: Callable[\n",
    "    [int], FlowerClient\n",
    "] = get_fedprox_flower_client_generator(\n",
    "    model_generator=network_generator_cnn,\n",
    "    data_dir=data_dir,\n",
    "    partition_dir=lda_partition,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K1N62Obm679"
   },
   "source": [
    "**Question 8 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. No code is needed)\n",
    "\n",
    "In FL, fairness is generally defined as the variance of the accuracy of the global model on local clients' test sets. If two models have the same average accuracy, the one with the lower variance between clients is the fairer.\n",
    "\n",
    "- In light of this, is it reasonable to say that analysing the fairness of the model across clients, i.e. the distributed accuracy, could be used as a tool to measure the data heterogeneity? Motivate your answer.\n",
    "- Both FedAvg and FedProx are training a single global model, so we will always measure some unfairness. Do you think we could improve fairness by finetuning the global model on the local clients just before evaluating it on their local test set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICOGgdnJRd1b"
   },
   "source": [
    "(c) 2024 Alexandru-Andrei Iacob, Lorenzo Sani\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "flwr-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f60f5a2c15992c74df12f0554524b987217e124a6a47cf1bc494002bece5a18b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
